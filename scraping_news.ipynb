{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c888d4b9-010f-4c62-b13c-ea5345e1f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snscrape tqdm pandas newsapi-python lxml[html_clean] newspaper3k -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2feec6c-57cd-41dc-af89-41602d8552ac",
   "metadata": {},
   "source": [
    "API KEY: febff2bff1eb4a359e887073c3166c0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b57a56c-e02b-4e46-97de-691d4d823aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing page 1: 100%|████████████████████████| 20/20 [00:16<00:00,  1.21it/s]\n",
      "Processing page 2: 100%|████████████████████████| 20/20 [00:13<00:00,  1.50it/s]\n",
      "Processing page 3: 100%|████████████████████████| 20/20 [00:13<00:00,  1.45it/s]\n",
      "Processing page 4: 100%|████████████████████████| 20/20 [00:06<00:00,  3.03it/s]\n",
      "Processing page 5: 100%|████████████████████████| 20/20 [00:08<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 42 new articles. Total saved: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from newsapi import NewsApiClient\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key = 'febff2bff1eb4a359e887073c3166c0a'\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "keywords = ['wildfire', 'avalanche', 'blizzard', 'heatwave', 'earthquake', 'flood', 'hurricane', 'drought', 'tsunami', 'landslide', 'tornado', 'volcano', 'natural disaster']\n",
    "query = ' OR '.join(f'\"{kw}\"' for kw in keywords)\n",
    "from_date = (datetime.now() - timedelta(days=28)).strftime('%Y-%m-%d')\n",
    "to_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "language = 'en'\n",
    "page_size = 20\n",
    "target_article_count = 60\n",
    "page = 1\n",
    "max_pages = 5\n",
    "new_articles = []\n",
    "existing_urls = set()\n",
    "\n",
    "output_file = 'disaster_news.json'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            existing_articles = json.load(f)\n",
    "            existing_urls = {article.get('url') for article in existing_articles if 'url' in article}\n",
    "        except json.JSONDecodeError:\n",
    "            existing_articles = []\n",
    "else:\n",
    "    existing_articles = []\n",
    "\n",
    "def is_blocked_url(url):\n",
    "    return \"consent.yahoo.com\" in url\n",
    "\n",
    "def extract_full_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text, article.authors\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "while len(new_articles) < target_article_count and page <= max_pages:\n",
    "    response = newsapi.get_everything(\n",
    "        q=query,\n",
    "        from_param=from_date,\n",
    "        to=to_date,\n",
    "        language=language,\n",
    "        sort_by='relevancy',\n",
    "        page_size=page_size,\n",
    "        page=page\n",
    "    )\n",
    "\n",
    "    articles = response.get('articles', [])\n",
    "    if not articles:\n",
    "        break\n",
    "\n",
    "    for article in tqdm(articles, desc=f\"Processing page {page}\"):\n",
    "        url = article['url']\n",
    "        if url in existing_urls or is_blocked_url(url):\n",
    "            continue\n",
    "        existing_urls.add(url)\n",
    "\n",
    "        title = article['title']\n",
    "        published_at = article['publishedAt']\n",
    "\n",
    "        if not any(kw.lower() in title.lower() for kw in keywords):\n",
    "            continue\n",
    "\n",
    "        full_text, authors = extract_full_text(url)\n",
    "\n",
    "        if full_text and len(full_text.split()) > 100:\n",
    "            article_data = {\n",
    "                'title': title,\n",
    "                'author': authors[0] if authors else None,\n",
    "                'date': published_at,\n",
    "                'url': url,\n",
    "                'content': full_text\n",
    "            }\n",
    "            new_articles.append(article_data)\n",
    "\n",
    "        if len(new_articles) >= target_article_count:\n",
    "            break\n",
    "\n",
    "    if len(articles) < page_size:\n",
    "        break\n",
    "\n",
    "    page += 1\n",
    "\n",
    "all_articles = existing_articles + new_articles\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Appended {len(new_articles)} new articles. Total saved: {len(all_articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334beca-76b8-4c09-bdb4-b16373553f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
